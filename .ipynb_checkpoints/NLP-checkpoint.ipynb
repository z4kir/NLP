{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d185042b-4751-44ce-83e4-d7d8cb153056",
   "metadata": {},
   "source": [
    "## Open Text File And Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141b7d70-94cb-46bb-86c0-6c844938e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) refers to the branch of computer science. The branch of artificial intelligence or AIâ€”concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\n"
     ]
    }
   ],
   "source": [
    "txt_file = open('text.txt')\n",
    "text = txt_file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d299c0a-8a14-46a1-8bf5-a4f0e2201c31",
   "metadata": {},
   "source": [
    "## Tokenize Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6be0f6-c6de-4867-8341-a0da8b08bc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) refers to the branch of computer science.', 'The branch of artificial intelligence or AIâ€”concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "# sentence tokenized\n",
    "sentence_tokenized = sent_tokenize(text)\n",
    "print(sentence_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e47eac0-b930-407e-8d44-ade1ee45c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science', '.', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'aiâ€', '”', 'concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "#word tokenized\n",
    "text_tokenized = word_tokenize(text.lower())\n",
    "print(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e179bf09-dd46-473e-b69f-02a408cfd802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove punctuations\n",
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73a5f166-e744-464d-983e-7f65227627e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing NLP refers to the branch of computer science The branch of artificial intelligence or AIâ€”concerned with giving computers the ability to understand text and spoken words in much the same way human beings can\n"
     ]
    }
   ],
   "source": [
    "text_clean = \"\".join([char for char in text if char not in punctuation ])\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddf106cb-da67-4913-8382-2e8cefc3eef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('to', 2), ('branch', 2), ('of', 2), ('.', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "#How many times any word is repeated\n",
    "fdist = FreqDist(text_tokenized)\n",
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d526be9-aff3-4e3a-bec9-fca19c2afd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'branch', 'computer', 'science', '.', 'The', 'branch', 'artificial', 'intelligence', 'AIâ€', '”', 'concerned', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'much', 'way', 'human', 'beings', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\humay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "text_nostop = ([char for char in text_tokenized if char not in stopwords])\n",
    "print(text_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5393085a-2210-48eb-ae44-1bcab269cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'refers', 'branch', 'computer', 'science', 'branch', 'artificial', 'intelligence', 'aiâ€', '”', 'concerned', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'much', 'way', 'human', 'beings']\n"
     ]
    }
   ],
   "source": [
    "#clean text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([char for char in text if char not in punctuation])\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    text = [char for char in tokens if char not in stopwords]\n",
    "    return text\n",
    "\n",
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7bf67-0f76-48af-89b7-3726924e368f",
   "metadata": {},
   "source": [
    "## Stammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8776dec-ecf9-4ef3-bc40-c515921dde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['study','studing','studies','studied','written','played']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4430c019-7d61-4adb-aeb1-5e9b5e4d7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# Chop the words wihtout going to dictionary\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ceb9823d-a0c7-46e4-927d-28e76369990d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi\n",
      "stude\n",
      "studi\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba5ac3-a130-43d5-a7b6-ee3193ec5bf3",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b49a9bf-1522-4e04-8b79-067a0023327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\humay\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# checks words in dictionary\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cb29dcd-9f2b-44c7-957a-a9b0670d8c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study\n",
      "stud\n",
      "study\n",
      "study\n",
      "write\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(lemma.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5181d6-b950-4fd6-bc6b-601b79f73933",
   "metadata": {},
   "source": [
    "## PoS Tagging\n",
    "parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03a41443-d251-4e2e-8dd2-1d1dfb77c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I lived in India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7b686fc-498f-4dca-ae16-44e94ed387e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\humay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\humay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b6386c84-e665-4e1a-ac97-7e6cb9bf41cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'lived', 'in', 'India']\n"
     ]
    }
   ],
   "source": [
    "text_token=word_tokenize(text)\n",
    "print(text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01fed699-a22d-413e-85b5-086da28b7fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('lived', 'VBD'), ('in', 'IN'), ('India', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "post_tags = pos_tag(text_token)\n",
    "print(post_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c30c4276-85a2-427b-92c8-98269c039197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('VBZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16931f5b-8198-42c9-a754-bc73abb31f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541dd8fe-31eb-4b82-a7de-e64cfea57000",
   "metadata": {},
   "source": [
    "## Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bc10a1e4-f48a-475a-bef0-f8cbcf1ec6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Taj Mahal is one of the wonder of world. Taj Mahal situated in Agra whihc is famous for two things one is taj mahal  and other is petha. \"\n",
    "# text = \"Vicky lives in Mumbai\"\n",
    "text = \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California. As of March 2023, Apple is the world's largest company by market capitalization, and with US$394.3 billion the largest technology company by 2022 revenue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab5d7f0c-ff09-4031-8305-c803c1b58a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "pos_tagged = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "31ff5f29-e540-4b8a-8c5d-0ed7e48d758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (GPE Vicky/NNP) lives/VBZ in/IN (GPE Mumbai/NNP))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\humay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\humay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "named_entities = nltk.ne_chunk(pos_tagged, binary=False)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "391610df-57b0-474c-b024-5b0582c5dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.7.2-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python310\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python310\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python310\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Using cached thinc-8.2.2-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python310\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python310\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python310\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python310\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python310\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python310\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python310\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python310\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python310\\lib\\site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\python310\\lib\\site-packages (from spacy) (1.26.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\humay\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\humay\\appdata\\roaming\\python\\python310\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\python310\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Using cached spacy-3.7.2-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "Using cached thinc-8.2.2-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Installing collected packages: thinc, spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python310\\\\Scripts\\\\spacy.exe' -> 'C:\\\\Python310\\\\Scripts\\\\spacy.exe.deleteme'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.6/12.8 MB 7.4 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.0/12.8 MB 8.1 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 9.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.2/12.8 MB 10.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.9/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.2/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.0/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 12.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.5/12.8 MB 12.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 13.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.6/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.6/12.8 MB 12.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.5/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.2/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.7/12.8 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 12.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\python310\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\humay\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\humay\\appdata\\roaming\\python\\python310\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\python310\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -0p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -1p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c732a3e1-f9f4-4a6e-acba-7cbc63345274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. - ORG\n",
      "American - NORP\n",
      "Cupertino - GPE\n",
      "California - GPE\n",
      "March 2023 - DATE\n",
      "Apple - ORG\n",
      "US$394.3 billion - MONEY\n",
      "2022 - DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,'-',ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab0580-c7b1-468c-a704-6bfeae7b09f4",
   "metadata": {},
   "source": [
    "## Beg of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e56a64b8-e4b3-49e1-83a1-bc6d03a5ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences= [\n",
    "    \"Jim and Pam travaled by bus. \",\n",
    "    \"The train was late. \",\n",
    "    \"The flight was full. Travelling by flight is expensive. \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9b464342-da64-4d8d-ab19-bed2b16af3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "839a793c-4157-401c-beaa-03cd80772617",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "#transform the sentence into a documnet-term matrix\n",
    "x = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "93e20769-b55f-49e8-8974-54722b67217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'bus' 'by' 'expensive' 'flight' 'full' 'is' 'jim' 'late' 'pam'\n",
      " 'the' 'train' 'travaled' 'travelling' 'was']\n"
     ]
    }
   ],
   "source": [
    "#Get the vocabulary (list of words)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "87689bbc-e883-4b00-ac52-e24abda43d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array = x.toarray()\n",
    "X_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7c49b-d916-4a0c-80f6-2b3e45e0b3c7",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b44472-2cd6-47c2-86f6-33e70ff27c18",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d1c67-1e8d-4fe9-ab3b-b5ecd54592a4",
   "metadata": {},
   "source": [
    "1.How often a term appears in a document.\n",
    "\n",
    "2.Calculate: TF = (Number of times term appears in documnet)/(Total number of terms in document).\n",
    "\n",
    "3.Example: In a document with 100 words, if a term appears 5 times. TF = 5/100 =0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ef787-27c0-4e6b-ba5b-ea80127fabde",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf5c87-d70d-40c5-9d74-df6af9c5caec",
   "metadata": {},
   "source": [
    "1.measeures how unique or rare a term across all documnets.\n",
    "\n",
    "2.Calculate: IDF = log(Total number of documents) / (Number of documents containing the term)\n",
    "\n",
    "3.Example: if there are 1,000 document and a term appears in 100 document, IDF = log(1000/100) = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5294a3b4-1d8e-4cd9-8f2e-b76c96903f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "67ff3907-1bcb-4bba-8731-f8a7843e14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample movie reviews\n",
    "documents = [\n",
    "    \"Jim and Pam travaled by bus. \",\n",
    "    \"The train was late. \",\n",
    "    \"The flight was full. Travelling by flight is expensive. \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44d72ab0-9bf9-4c72-928f-76282e957cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the document to calculate TF-IDF scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0731f562-04bf-43d9-9748-2ecd4a6257aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aef4f908-ac32-490e-bb9c-922353036d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.42339448 0.42339448 0.32200242 0.         0.         0.\n",
      "  0.         0.42339448 0.         0.42339448 0.         0.\n",
      "  0.42339448 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5628291  0.         0.42804604 0.5628291\n",
      "  0.         0.         0.42804604]\n",
      " [0.         0.         0.24374827 0.32049968 0.64099936 0.32049968\n",
      "  0.32049968 0.         0.         0.         0.24374827 0.\n",
      "  0.         0.32049968 0.24374827]]\n"
     ]
    }
   ],
   "source": [
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7f9d4e2c-c9c0-4934-8ba9-65428dbaa115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n",
      "['and' 'bus' 'by' 'expensive' 'flight' 'full' 'is' 'jim' 'late' 'pam'\n",
      " 'the' 'train' 'travaled' 'travelling' 'was']\n"
     ]
    }
   ],
   "source": [
    "# Display the features names\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ad4bd-342b-44a6-93b6-bfbb46fc2c3b",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18c4c4-a409-432d-8476-430e9ab12eb7",
   "metadata": {},
   "source": [
    "Search Engines:How search engines rank documnet based on relevance to user queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c50ca0-bf67-4e34-8a45-413b6112e851",
   "metadata": {},
   "source": [
    "Text Summarization:Identifying key terms for creating concise summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf5f96-8f78-4d64-8e83-8287fa07d7df",
   "metadata": {},
   "source": [
    "Document Classification:Assisting in categorizing documents based on their content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d4591-de43-4bdb-928b-f02fc5d1ea56",
   "metadata": {},
   "source": [
    "### Limitaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c4aa1-d4b9-456d-bd8a-fcd11524fe68",
   "metadata": {},
   "source": [
    "TF-IDF dosen't consider the context or meaning of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe0f87-341a-471e-9bd9-1899409cd7f9",
   "metadata": {},
   "source": [
    "Prone to high-frequency terms dominating the importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17b01d-a8fe-4525-a704-0a89e0de3b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
