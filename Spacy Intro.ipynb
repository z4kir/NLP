{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b94463f-16e4-47eb-84e5-e461b044574b",
   "metadata": {},
   "source": [
    "# spaCy Tutorial - NLP with python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2580b-dd8e-4359-8bab-bf263d2c3418",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9635c69-0b22-4e28-a6d2-03dde6af30a0",
   "metadata": {},
   "source": [
    "Spacy is popular open-source natural language processing(NLP) library that is designed to be fast, efficient, and prodction-ready. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d1bb7-6bc9-44c8-97a8-5b9480a38370",
   "metadata": {},
   "source": [
    "## Basic NLP Tasks with spaCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eca5fb6-7aa5-454e-a850-c9702f407075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n",
      "India PROPN\n",
      "is AUX\n",
      "beautiful ADJ\n",
      "country NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"India is beautiful country.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e44c0-89a3-4138-8040-28f900d339a1",
   "metadata": {},
   "source": [
    "## Named Entity Recognition(NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c02d9421-2c27-4e59-b972-f07d572f1da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d6d1ea-840e-40a6-916e-106f0216d30a",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "195dd76f-6f0e-4468-b00d-2805d1eff417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India nsubj is AUX\n",
      "is ROOT is AUX\n",
      "beautiful amod country NOUN\n",
      "country attr is AUX\n",
      ". punct is AUX\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_,token.head.text,token.head.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76f25e94-6c9d-4d9d-bfa4-a3c5b8b129ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjectival modifier\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('amod'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113324d0-7782-4d2c-8552-ba77d280d84e",
   "metadata": {},
   "source": [
    "## Advanced NLP Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782336e3-dcca-4ca4-a2d6-c06ae4b7f016",
   "metadata": {},
   "source": [
    "#### word vectors and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9eddaad5-2de3-44a2-8ecc-a2105851e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.7899729502051105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\humay\\AppData\\Local\\Temp\\ipykernel_12216\\4077908094.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = word1.similarity(word2)\n"
     ]
    }
   ],
   "source": [
    "word1 = nlp(\"king\")\n",
    "word2 = nlp(\"prince\")\n",
    "# word2 = nlp(\"queen\")\n",
    "similarity = word1.similarity(word2)\n",
    "print(\"Similarity:\",similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88cff60-6513-47ad-b1ec-ee19ba967fed",
   "metadata": {},
   "source": [
    "## Text Preprocessing With spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb6f31d4-1d9a-489d-87a1-3de957cac13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am running in the park\n"
     ]
    }
   ],
   "source": [
    "text = \"I am running in the park\"\n",
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a783e8-abe9-41ac-85e3-3d6aaaab384e",
   "metadata": {},
   "source": [
    "## Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de322221-929c-4ec7-b2ea-dcde54fbd684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'running', 'in', 'the', 'park']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a8f80-4a9b-4436-b0ac-28eeb274a702",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bed5f454-0d7f-4dcb-9ffc-b1fbf4db5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61df4665-880c-4fc1-9408-f409166e50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'be', 'run', 'in', 'the', 'park']\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec7013-4dd7-4ba1-ae9b-e353595e3c0c",
   "metadata": {},
   "source": [
    "## Removing stopword and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f15abd5-cd0e-4da7-8638-3e549c3cacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'park']\n"
     ]
    }
   ],
   "source": [
    "filterd_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "print(filterd_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031c2b7-c84c-4800-8764-656190366813",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "998d9511-1044-4b21-9bd9-6eebc8761e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRON'), ('am', 'AUX'), ('running', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('park', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = [(token.text,token.pos_)for token in doc]\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79cc22-9b4f-49e9-aff5-7f8f74a699b2",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbd27f3e-901a-4792-83e4-f39193eadec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I lived in India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e289cf8-8f86-4c1f-9052-9f483ccbdb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('India', 384)]\n"
     ]
    }
   ],
   "source": [
    "entities = [(ent.text,ent.label) for ent in doc1.ents]\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda6dc6-5edd-4825-8970-51ee29cc38f8",
   "metadata": {},
   "source": [
    "## TextBlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "476e9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60ba9288",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The brown dog chased the cat. Rahul is a bad boy\"\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6c23050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'dog', 'chased', 'the', 'cat', 'Rahul', 'is', 'a', 'bad', 'boy']\n",
      "[Sentence(\"The brown dog chased the cat.\"), Sentence(\"Rahul is a bad boy\")]\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "words = blob.words\n",
    "print(words)\n",
    "sentences = blob\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4481ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('dog', 'NN'), ('chased', 'VBD'), ('the', 'DT'), ('cat', 'NN'), ('Rahul', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('bad', 'JJ'), ('boy', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# pos tagging \n",
    "pos_tags = blob.tags\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b49e7190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['brown dog', 'rahul', 'bad boy'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noun phrases\n",
    "noun_phrases = blob.noun_phrases\n",
    "noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f3ee4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"i have a book\")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spell Checker\n",
    "sentence = \"i havv a book\"\n",
    "obj = TextBlob(sentence)\n",
    "obj.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3ded91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'books'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.words[3].pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "038f43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I hate this country\"\n",
    "obj = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a1a4159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.8, subjectivity=0.9)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88f42f",
   "metadata": {},
   "source": [
    "## N-Grams in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d2ade",
   "metadata": {},
   "source": [
    "in nlp an n-gram is a contiguous sequence of n items(or tokens) from a given sample of text of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f87663c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('this', 'is'), ('is', 'a'), ('a', 'sentence')]\n",
      "\n",
      "trigrams: [('this', 'is', 'a'), ('is', 'a', 'sentence')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.util import ngrams\n",
    "\n",
    "sentence = \"this is a sentence\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "bigrams = list(ngrams(tokens,2))\n",
    "trigrams = list(ngrams(tokens,3))\n",
    "\n",
    "print('Bigrams:',bigrams)\n",
    "print()\n",
    "print('trigrams:',trigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649d25a",
   "metadata": {},
   "source": [
    "## Application of the   N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b02aff",
   "metadata": {},
   "source": [
    "Language modeling\n",
    "\n",
    "Text classification\n",
    "\n",
    "spell checking\n",
    "\n",
    "Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c22479",
   "metadata": {},
   "source": [
    "## N-grams in Language Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a00e2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE # Maximum Likelihood Estimation (MLE)\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2c660",
   "metadata": {},
   "source": [
    "<b>Padded_everygram_pipeline</b> = padd_sequence + everygram\n",
    "\n",
    "<b>padd_sequence</b>: padding adds special tokens (like &lt;s> for the start of a sentence and &lt;/s> for the end) to make all sentences the same length.\n",
    "\n",
    "<b>everygrams</b>: is funciton that helps you generate all possible n-grams(contiguous sequences of n items, usally words) from a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5da0678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: l a n g u a g e\n"
     ]
    }
   ],
   "source": [
    "text = \"I love working natural language processing. I love nlp most.\"\n",
    "sentence = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
    "\n",
    "#Create a vocabulory and preprocess the text\n",
    "n = 3 # Use trigrams\n",
    "train_data, padded_sents = padded_everygram_pipeline(n,sentence)\n",
    "\n",
    "model = MLE(n) # create trigram model\n",
    "model.fit(train_data,padded_sents)\n",
    "\n",
    "# Generate the next word\n",
    "next_word = model.generate(1,random_seed=42)\n",
    "print(\"Generated text:\", \" \".join(next_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af496199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: working natural language\n"
     ]
    }
   ],
   "source": [
    "# Previous word\n",
    "previous_word = \"love\"\n",
    "\n",
    "#Generate text the next word based on the previous word\n",
    "next_word = model.generate(3,random_seed=42,text_seed=[previous_word])\n",
    "print(\"Generated text:\",\" \".join(next_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8629ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
